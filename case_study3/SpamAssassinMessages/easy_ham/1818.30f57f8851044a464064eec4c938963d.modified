{"header": "return path tim one comcast net deliveri date sun sep 8 08 18 49 2002 from tim one comcast net tim peter date sun 08 sep 2002 03 18 49 0400", "body": "subject spambay test set in repli to 200209080538 g885cjk17553 pcp02138704pc reston01 va comcast net messag id lnbbljkpbehfedalkolcgeoibcab tim one comcast net guido i meant to say that they were 0 99 clue cancel out by 0 01 clue but that s wrong too it look i haven t grok thi part of your code yet thi one ha way more than 16 clue and it seem the classifi basic end up count way more 0 99 than 0 01 clue and no other made it into the list i thought it wa look for clue with valu in between appar it found none that weren t exactli 0 5 there s a brief discuss of thi befor the definit of max discrimin all clue with prob min spamprob and max spamprob are save in min and max list and all other clue are fed into the nbest heap then the shorter of the min and max list cancel out the same number of clue in the longer list whatev remain of the longer list if anyth is then fed into the nbest heap too but no more than max discrimin of them in no case do more than max discrimin clue enter into the final probabl calcul but all of the min and max list go into the list of clue els you d have no clue that massiv cancel wa occur and massiv cancel may yet turn out to be a hook to signal that manual review is need in your specif case the excess of clue in the longer max spamprob list push everyth els out of the nbest heap and that s whi you didn t see anyth other than 0 01 and 0 99 befor ad these special list the outcom when face with mani 0 01 and 0 99 clue wa too often a coin toss whichev flavor just happen to appear max discrimin 2 1 time first determin the final outcom that sure set the record for longest list of cancel extrem clue thi happen to be the longest one but there were quit a few similar one i just beat it wink a token scheme that fold case and ignor punctuat and strip a trail s from word and save both word bigram and word unigram turn up a low probabl veri long spam with a list of 410 0 01 clue and 125 0 99 clue yike i wonder if there s anyth we can learn from look at the clue and the html it wa heavili mark up html with ad in the sidebar but the bodi text wa a seriou discuss of oo and soft code with lot of highli technic word as clue includ zope and zeo no matter how often it say zope it get onli one 0 01 clue from do so ditto for zeo in contrast html markup ha mani uniqu word that get 0 99 btw thi is a clear case where the assumpt of condit independ word probabl is utterli bogu e g the probabl that bodi appear in a messag is highli correl with the prob of br appear by treat them as independ naiv bay grossli misjudg the probabl that both appear and the onli thing you get in return is someth that can actual be comput wink read the what about html section in token py from the veri start i ve been investig what would work best for the mail list host at python org and html decor have so far been too strong a clue to justifi ignor it in that specif context i haven t done anyth gear toward person email includ the case of non mail list email that happen to go through python org i d prefer to strip html tag from everyth but last time i tri that it still had bad effect on the error rate in my corpora the full test result with and without html tag strip is includ in the what about html comment block but as the comment block also say xxx so if anoth way is found to slash the f n rate the decis here xxx not to strip html from html onli msg should be revisit and we ve sinc done sever thing that gave signific f n rate reduct i should test that again now are there ani minabl but unmin header line in your corpu left almost all of them apart from mime decor that appear in both header and bodi like content type the onli header line the base token look at now are subject from x mailer and organ or do we have to start with a differ corpu befor we can make progress there i would need differ data ye my ham is too pollut with mailman header decor which i may or may not be abl to clean out but fudg the data is a mortal sin and i haven t chang a byte so far and my spam too pollut with header clue about the fellow who collect it in particular i have to skip to and receiv header now and i suspect they re go to be veri valuabl in real life for exampl i don t even catch undisclos recipi in the to header now no sorri these were all of the follow structur multipart mix text plain brief text plu url s text html long html copi from websit ah that explain whi the html tag didn t get strip i d again offer to add an option argument to token so that they d get strip here too but if it get gloss over a third time that would feel too much like a loss wink thi seem confus jeremi didn t use my train classifi pickl he train hi own classifi from scratch on hi own corpora i think it s still corpu size i report on test i ran with random sampl of 220 spam and 220 ham from my corpu that mean train on set of those size as well as predict on set of those size and while that did harm the error rate the error rate i saw were still much better than jeremi report when use 500 of each ah a full test run just finish on the token scheme that fold case and ignor punctuat and strip a trail s from word and save both word bigram and word unigram thi is the code token everyth in the bodi lastw for w in word re findal text n len w make sure thi rang match in token word if 3 n 12 if w 1 s w w 1 yield w if lastw yield lastw w lastw w elif n 3 lastw for t in token word w yield t where word re re compil r w x80 xff thi at least doubl the process size over what s done now it help the f n rate significantli but probabl hurt the f p rate the f p rate is too low with onli 4000 ham per run to be confid about chang of such small absolut magnitud 0 025 is a singl messag in the f p tabl fals posit percentag 0 000 0 000 tie 0 000 0 075 lost wa 0 0 050 0 125 lost 150 00 0 025 0 000 won 100 00 0 075 0 025 won 66 67 0 000 0 050 lost wa 0 0 100 0 175 lost 75 00 0 050 0 050 tie 0 025 0 050 lost 100 00 0 025 0 000 won 100 00 0 050 0 125 lost 150 00 0 050 0 025 won 50 00 0 050 0 050 tie 0 000 0 025 lost wa 0 0 000 0 025 lost wa 0 0 075 0 050 won 33 33 0 025 0 050 lost 100 00 0 000 0 000 tie 0 025 0 100 lost 300 00 0 050 0 150 lost 200 00 won 5 time tie 4 time lost 11 time total uniqu fp went from 13 to 21 fals neg percentag 0 327 0 218 won 33 33 0 400 0 218 won 45 50 0 327 0 218 won 33 33 0 691 0 691 tie 0 545 0 327 won 40 00 0 291 0 218 won 25 09 0 218 0 291 lost 33 49 0 654 0 473 won 27 68 0 364 0 327 won 10 16 0 291 0 182 won 37 46 0 327 0 254 won 22 32 0 691 0 509 won 26 34 0 582 0 473 won 18 73 0 291 0 255 won 12 37 0 364 0 218 won 40 11 0 436 0 327 won 25 00 0 436 0 473 lost 8 49 0 218 0 218 tie 0 291 0 255 won 12 37 0 254 0 364 lost 43 31 won 15 time tie 2 time lost 3 time total uniqu fn went from 106 to 94"}